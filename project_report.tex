\documentclass[11pt,a4paper]{article}

\usepackage[margin=2.5cm]{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{graphicx}

\setstretch{1.15}

\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

\title{Mode Connectivity in Neural Networks: \\ Understanding Loss Landscapes \& Prediction Stability \\ \vspace{0.3cm}}
\author{Maciek Lodzinski}
\date{November 26, 2025}

\begin{document}

\maketitle

\begin{abstract}
This project investigates mode connectivity in neural networks, analyzing whether independently trained VGG16 models on CIFAR-10 can be connected by low-loss paths. We trained Bezier curves with and without L2 regularization, tested neuron permutation effects, implemented polygon chain optimization, and developed a novel symmetry plane constrained optimization method. Key findings: (1) Regularization reduces test error barriers (7.75\% â†’ 6.84\%), (2) Neuron permutations significantly affect connectivity (swapping at epoch 2 produces 18.8\% barrier vs 6.84\% without swaps), (3) Symmetry plane constrained paths achieve similar performance to unconstrained polygon chains while maintaining geometric constraints to machine precision ($\sim10^{-8}$), and (4) 14.82\% of test samples change predictions along curves, with class-dependent stability.
\end{abstract}

\section{Research Questions}

\textbf{Primary:} Can independently trained neural networks be connected by low-loss continuous paths in parameter space?

\textbf{Key questions addressed:}
\begin{itemize}
    \item Does L2 regularization during curve training improve connectivity?
    \item How do neuron permutations (from permutation symmetries) affect mode connectivity?
    \item Can we constrain optimization to symmetry plane hyperplanes while maintaining low-loss paths?
    \item How do predictions change along curves? Which samples and classes are most unstable?
\end{itemize}

\section{Experimental Setup}

\textbf{Model:} VGG16 on CIFAR-10 (10 classes, 10,000 test samples)

\textbf{Endpoints:} Three independently trained models at epoch 200:
\begin{itemize}
    \item seed0: 93.01\% test accuracy
    \item seed1: 93.06\% test accuracy
    \item seed0\_mirrored: Sign-flipped weights from seed0
\end{itemize}

\textbf{Methods compared:}
\begin{itemize}
    \item \textbf{Bezier curves (3-bend):} With/without L2 regularization ($\lambda=10^{-4}$)
    \item \textbf{Neuron swap experiments:} Permuting filter indices at epochs 2, 100, 200
    \item \textbf{Polygon chain:} Piecewise linear path with 3 bends (no constraints)
    \item \textbf{Symmetry plane:} Polygon chain constrained to hyperplane between endpoints
\end{itemize}

\textbf{Evaluation:} All paths evaluated at 61 points ($t \in [0, 1]$) with batch normalization statistics updated. Maximum test error (barrier height) used as primary connectivity metric.

\section{Key Findings}

\subsection{Curve Training With and Without Regularization}

We trained 3-bend Bezier curves between seed0-seed1 and seed0-mirror endpoints with and without L2 regularization ($\lambda=10^{-4}$).

\textbf{Results (seed0 $\leftrightarrow$ seed1):}
\begin{itemize}
    \item \textbf{Without regularization:} Max test error = 7.75\% (train error = 0.14\%)
    \item \textbf{With regularization:} Max test error = 6.84\% (train error = 0.24\%)
    \item \textbf{Improvement:} Regularization reduces barrier height by 0.91 percentage points (11.7\% relative reduction)
\end{itemize}

\textbf{Results (seed0 $\leftrightarrow$ seed0\_mirrored):}
\begin{itemize}
    \item \textbf{Without regularization:} Successfully connected with low barrier
    \item \textbf{With regularization:} Similar performance, confirms regularization benefit
\end{itemize}

\textbf{Key observation:} Unregularized curves achieve near-zero training error (0.14\%) but show larger test barriers, indicating overfitting to the training manifold. Regularization trades slightly higher training error for better test generalization along the path.

\subsection{Neuron Swap Experiments}

We tested how neuron permutations affect mode connectivity by randomly swapping filter indices at different training stages (epochs 2, 100, 200) in one endpoint before curve training.

\textbf{Results (seed0 $\leftrightarrow$ seed1\_swapped):}
\begin{itemize}
    \item \textbf{Early swap (epoch 2):} Max test error = 18.8\% -- \textit{severe barrier}
    \item \textbf{Mid swap (epoch 100):} Max test error $\sim$12\% -- \textit{moderate barrier}
    \item \textbf{Late swap (epoch 200):} Max test error $\sim$8\% -- \textit{small barrier}
    \item \textbf{No swap (baseline):} Max test error = 6.84\%
\end{itemize}

\textbf{Key finding:} Neuron permutations break mode connectivity, with effects depending on when swaps occur. Early-stage permutations (epoch 2) create the largest barriers (18.8\%), as networks haven't yet learned similar feature representations. This confirms that mode connectivity relies on aligned neuron correspondences between networks, not just final function similarity.

\subsection{Symmetry Plane Constrained Optimization}

We developed a novel method constraining polygon chain middle points to the symmetry plane hyperplane between endpoints, defined by $\mathbf{n} \cdot (\theta - \mathbf{m}) = 0$ where $\mathbf{n} = w_1 - w_0$ and $\mathbf{m} = (w_0 + w_1)/2$.

\textbf{Implementation:} After each gradient step, project middle bend: $\theta_{\text{new}} = \theta - \frac{(\theta - \mathbf{m}) \cdot \mathbf{n}}{\|\mathbf{n}\|^2} \mathbf{n}$

\textbf{Verification results:}
\begin{itemize}
    \item \textbf{Polygon chain (unconstrained):} Normalized distance = 2.56 (\textit{not on plane})
    \item \textbf{Symmetry plane:} Normalized distance = $7.2 \times 10^{-8}$ (\textit{on plane to machine precision})
    \item \textbf{Constraint strength:} $\sim$35 million times stronger than unconstrained
\end{itemize}

\textbf{Performance comparison (seed0 $\leftrightarrow$ seed0\_mirrored):}
\begin{itemize}
    \item Polygon chain: Max test error $\sim$7\%
    \item Symmetry plane: Max test error $\sim$7\%
    \item \textbf{Result:} Nearly identical performance despite geometric constraint
\end{itemize}

\textbf{Key insight:} Constraining optimization to the symmetry plane hyperplane does not degrade path quality. This suggests the optimal paths naturally lie close to or can be effectively represented within this high-dimensional hyperplane, providing a geometric interpretation of mode connectivity.

\subsection{Prediction Stability Analysis}

Analysis of seed0 $\leftrightarrow$ seed1 curve (no regularization) reveals:

\textbf{Overall:} 14.82\% (1,482/10,000) test samples change predictions along path, with U-shaped accuracy profile (minimum 90.3\% at $t=0.533$).

\textbf{Class-dependent stability:}
\begin{itemize}
    \item \textbf{Most unstable:} cat (29.9\%), dog (20.2\%), bird (19.8\%), airplane (18.9\%)
    \item \textbf{Most stable:} automobile (5.4\%), truck (8.2\%), horse (9.4\%), frog (10.6\%)
\end{itemize}

\textbf{Interpretation:} Vehicle classes maintain $>$94\% accuracy throughout due to distinctive geometric features. Animal classes show 4-9\% accuracy drops due to shared textures and poses, suggesting paths traverse near inter-class decision boundaries.

\section{Additional Implementations}

Beyond the main experiments, several supporting components were developed:

\begin{itemize}
    \item \textbf{Symmetry plane verification:} Mathematical verification script computing normalized distance to hyperplane, confirming constraint satisfaction to $\sim10^{-8}$ precision
    \item \textbf{Evaluation infrastructure:} CPU/MPS-compatible evaluation scripts for local testing, plus SLURM cluster scripts for batch GPU evaluation
    \item \textbf{Configuration management:} Hydra-based Python wrappers with YAML configs for reproducible experiments with consistent seed management
    \item \textbf{Visualization pipeline:} Automated workflow for per-class accuracy curves, UMAP scatter plots, and animated GIFs of prediction evolution
    \item \textbf{Feature extraction:} Modified evaluation scripts to extract penultimate layer features from endpoints for representation analysis
\end{itemize}

\section{Open Questions and Future Work}

\textbf{Theoretical:}
\begin{itemize}
    \item Why does symmetry plane constraint not degrade path quality?
    \item Can we predict barrier height from endpoint similarity metrics?
    \item What is the relationship between neuron alignment and barrier height?
\end{itemize}

\textbf{Experimental:}
\begin{itemize}
    \item Compare to linear interpolation baseline (quantify benefit of curves)
    \item Test on different architectures (ResNet, Transformer) and datasets (CIFAR-100, ImageNet)
    \item Analyze training dynamics: how does connectivity evolve during endpoint training?
    \item Feature space analysis: do changing samples cluster near decision boundaries?
\end{itemize}

\section{Summary of Findings}

\textbf{Mode connectivity confirmed:} VGG16 models trained from different random seeds can be connected by low-loss Bezier curves, with regularization reducing barriers by 11.7\%.

\textbf{Neuron permutations break connectivity:} Random neuron swaps create barriers proportional to when they occur (epoch 2: 18.8\%, epoch 200: 8\%), confirming connectivity requires neuron alignment.

\textbf{Symmetry plane paths are viable:} Constraining polygon chains to symmetry hyperplanes maintains identical performance to unconstrained optimization, suggesting optimal paths naturally lie near this geometric structure.

\textbf{Prediction instability is class-dependent:} 14.82\% of samples change predictions along paths, with animal classes (20-30\% unstable) far more sensitive than vehicles (5-8\%), reflecting visual feature distinctiveness.

\section{Useful papers}

\begin{itemize}
    \item \textbf{Garipov et al. (2018):} ``Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs'' -- Original mode connectivity work, introduced Bezier curve training
    \item \textbf{Draxler et al. (2018):} ``Essentially No Barriers in Neural Network Energy Landscape'' -- Showed that barriers between minima are often surmountable
    \item \textbf{Fort \& Jastrzebski (2019):} ``Large Scale Structure of Neural Network Loss Landscapes'' -- Analyzed global geometry of loss surfaces
    \item \textbf{Frankle et al. (2020):} ``Linear Mode Connectivity and the Lottery Ticket Hypothesis'' -- Showed pruned networks can be linearly connected
\end{itemize}

\end{document}
